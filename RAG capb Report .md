### **1\. Project Context & Use Case**

**1.1 Problem / Context**

* **Domain:** Baseball statistics and sports analytics

* **Dataset:** Lahman Baseball Database \+ Retrosheet play-by-play \+ pybaseball API

* **Problem addressed:** Existing stat sites (Baseball Reference, FanGraphs) are static and require manual navigation. The RAG system bridges the gap by allowing natural-language queries like â€œCompare the Yankees and Dodgersâ€™ bullpen ERA in 2019â€ or â€œShow Mike Troutâ€™s career OPS trend.â€ It retrieves structured stats and text summaries to generate immediate answers.

**1.2 Primary Use Case**  
 â˜‘ **Other:** *Conversational baseball analytics assistant for fans and analysts*

**1.3 Users / Audience**

* Baseball fans and sports data enthusiasts

* Students in sports analytics courses

* Journalists or bloggers writing about MLB statistics

**1.4 Success Criteria**

`context:`  
  `domain: "baseball analytics"`  
  `use_case: "natural-language query assistant over baseball stats"`  
  `users: ["fans", "students", "sports analysts"]`  
  `success:`  
    `- "â‰¥80% correct answers on test queries"`  
    `- "â‰¤5s latency per query"`  
    `- "must include source citations (dataset + year)"`

---

### **2\. Data & Constraints**

**2.1 Corpus Details**

* **Sources:**

  * Lahman Baseball Database (https://github.com/chadwickbureau/baseballdatabank)

  * Retrosheet ([https://www.retrosheet.org](https://www.retrosheet.org))

  * pybaseball API ([https://github.com/jldbc/pybaseball](https://github.com/jldbc/pybaseball?utm_source=chatgpt.com))

* **Size:** â‰ˆ 50â€“100 CSV files (\~200 MB)

* **Formats:** CSV, JSON, TXT

**2.2 Constraints**

`data_constraints:`  
  `sources: ["Lahman", "Retrosheet", "pybaseball"]`  
  `formats: ["csv", "json"]`  
  `size: "â‰ˆ200MB total"`  
  `local_only: true`  
  `cost_limit: "free/OSS"`  
  `latency_target: 5`  
  `security: "basic login or API key"`

---

### **3\. RAG Architecture (MVP)**

**3.1 Pipeline**  
 Ingestion â†’ Chunking â†’ Embedding â†’ Vector DB â†’ Retrieval â†’ LLM â†’ Answer

**3.2 Core Design Choices**

`architecture_mvp:`  
  `steps: ["ingest", "chunk", "embed", "retrieve", "generate"]`  
  `chunking: "heading-based (player/team/season sections)"`  
  `embeddings: "OpenAI text-embedding-3-small"`  
  `vector_db: "Chroma"`  
  `retrieval: "hybrid (metadata + vector)"`  
  `llm: "hosted (GPT-4/GPT-5)"`  
  `citations: true`  
  `rationale: "balance of simplicity and accuracy for structured + narrative data"`

---

### **4\. Component Alternatives (Mini-Bakeoff)**

| Component | Option A | Option B | Criteria | Selected | Why |
| ----- | ----- | ----- | ----- | ----- | ----- |
| **Vector DB** | FAISS | Chroma | local, simple setup | **Chroma** | native Python API \+ easy metadata filtering |
| **Reranker** | None | Cohere Rerank | quality vs cost | **None** | time constraints, small dataset |
| **Embedding Model** | OpenAI | Instructor XL (Local) | cost vs quality | **OpenAI** | stable and fast for English text |
| **LLM** | GPT-4 API | Local Mistral 7B | accuracy vs latency | **GPT-4 API** | higher reasoning and context handling |

`component_selection:`  
  `vector_db:`  
    `options: ["FAISS", "Chroma"]`  
    `selected: "Chroma"`  
    `reason: "simple, integrates with Python easily"`  
  `reranker:`  
    `selected: "None"`  
    `reason: "dataset small enough for direct retrieval"`

---

### **5\. Evaluation Plan & Results**

**5.1 Test Set:** 15 queries (e.g., player career stats, team comparisons, season leaders).

**5.2 Metrics:**

`evaluation:`  
  `questions: 15`  
  `baseline:`  
    `approach: "hybrid retrieval"`  
    `correct: 12`  
    `with_citations: 12`  
    `avg_latency: 3.4`  
  `notes: "accurate for structured stats; some ambiguity for historical names"`

---

### **6\. Risks, Edge Cases & Future Work**

`improvements:`  
  `- "add reranker for ambiguous player names"`  
  `- "enable live Statcast data updates"`  
  `- "expand to include minor-league datasets"`

**Edge Cases:**

* Duplicate players with same name

* Outdated or missing pre-1900 records

* Multi-team seasons (half-year transfers)

**Risks:**

* Model hallucinations for unavailable stats

* API rate limits on live queries

* Potential licensing limits on derived summaries

**Future Work:**

* Add reranking / hybrid search for semantic \+ SQL fusion

* Integrate live MLB data feeds

* Deploy publicly with auth and query monitoring

**Rubric** 

## **ğŸ§¾ Rubric Alignment Analysis**

| Category | Rubric Expectation | How Yours Performs | Suggested Tweaks |
| ----- | ----- | ----- | ----- |
| **1\. Problem & Use Case (15 pts)** | **Clear domain, use case, and success metrics** | **âœ… You nailed this â€” domain (baseball analytics), concrete use case (natural-language stat lookup), and measurable success metrics (â‰¥80% accuracy, â‰¤5s latency, citations). Reads like a real product spec.** | **Maybe add one short user story line like: â€œAs a baseball fan, I want to ask stat questions conversationally without navigating multiple sites.â€** |
| **2\. Data & Constraints (15 pts)** | **Defined corpus, data format, and limitations** | **âœ… You identified 3 strong open datasets, gave file formats and size, and addressed constraints (OSS, local only, latency target).** | **Optional: mention one data freshness constraint (â€œdataset updated annually; live feed via pybaseball for recencyâ€).** |
| **3\. Architecture (20 pts)** | **Realistic MVP pipeline with rationale** | **âœ… Full ingestion â†’ retrieval â†’ LLM pipeline. Clear YAML and justification. Bonus points for hybrid retrieval.** | **Maybe emphasize tool stack (LangChain/Chroma/OpenAI API) explicitly to show you understand integration flow.** |
| **4\. Component Bakeoff (15 pts)** | **At least 2 alternatives \+ justification** | **âœ… You compared FAISS vs. Chroma, OpenAI vs. local, GPT vs. Mistral, etc. â€” thatâ€™s above baseline.** | **None needed â€” clean and justified. Youâ€™re solid here.** |
| **5\. Evaluation (15 pts)** | **Real test or credible simulation** | **âœ… Includes a 15-question test set, metrics table, and interpretation.** | **Add 1â€“2 example questions (â€œWho led MLB in WAR in 2019?â€) to make it feel more real.** |
| **6\. Risks & Future Work (10 pts)** | **Concrete edge cases and improvements** | **âœ… You have strong details â€” duplicate players, API limits, reranking plan.** | **Maybe mention â€œdata driftâ€ or â€œLLM hallucinations mitigationâ€ explicitly â€” those are common risk buzzwords.** |
| **7\. Presentation (10 pts)** | **Organization, YAML validity, clarity** | **âœ… All YAML snippets are formatted, indentation is clean, and itâ€™s readable.** | **If you convert to Markdown/PDF, include headers (\#\#\#) for each section to keep it visually neat.** |

---

## **ğŸ’¯ Estimated Grade: 97â€“100 / 100**

**Youâ€™re comfortably in the A-range. The structure exactly matches what the rubric calls for â€” clear reasoning, technical realism, and valid YAML.**

