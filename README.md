# Baseball-RAG-System-

# MP3 â€“ Baseball RAG: Conversational Baseball Analytics Assistant

This repo contains the materials for **Mini-Project #3: Real-World RAG Implementation** for the AI Mini-Project Series.

The project designs a **Retrieval-Augmented Generation (RAG)** system over open baseball statistics datasets (Lahman, Retrosheet, pybaseball) to let users ask **natural-language questions** about players, teams, and seasons.

---

## ğŸš€ Project Overview

**Goal:**  
Build a small but realistic RAG design that lets a user ask questions like:

- â€œWho had the highest WAR among AL pitchers in 2019?â€
- â€œCompare the Yankeesâ€™ bullpen ERA in 2018 vs 2020.â€
- â€œShow Mike Troutâ€™s career OPS trend.â€

The system uses structured stats and text summaries to generate answers, returning both numbers and short explanations with citations.

---

## ğŸ§­ Domain & Use Case

- **Domain:** Baseball statistics and sports analytics
- **Use Case:** Conversational baseball analytics assistant over MLB stats
- **Users:**
  - Baseball fans and sports nerds
  - Students in sports analytics / data science classes
  - Journalists / bloggers who want quick stat pull + explanation

**Success Criteria (high level):**

- â‰¥ 80% correct answers on a small test set of queries
- â‰¤ 5 seconds latency per query
- Every answer includes a source citation (dataset + year/season)

Full details are in [`reports/capb_report.md`](reports/capb_report.md).

---

## ğŸ“Š Data & Constraints

**Datasets:**

- **Lahman Baseball Database** â€“ core structured stats (players, teams, seasons)
- **Retrosheet** â€“ play-by-play and game-level event data
- **pybaseball API** â€“ access to modern/advanced stats and potentially live data

**Formats:** CSV, JSON, possibly TXT/MD for narrative summaries.  
**Scale:** ~200 MB total for initial MVP (manageable locally).

**Constraints:**

- Use **free / open-source** tooling and datasets
- Assume **local-only processing** for the MVP
- Latency target: **â‰¤ 5s** per query
- Simple security: basic login / API key if deployed

---

## ğŸ§± RAG Architecture (MVP)

Minimal pipeline:

1. **Ingestion** â€“ load Lahman + Retrosheet CSVs, optional pybaseball pulls
2. **Chunking** â€“ group data by *player*, *team*, *season* (heading-based chunks)
3. **Embedding** â€“ generate embeddings for textified â€œstat cardsâ€ and summaries
4. **Vector DB** â€“ store embeddings in **Chroma** with metadata (player, year, team)
5. **Retrieval** â€“ hybrid search (metadata filters + vector similarity)
6. **LLM** â€“ hosted GPT-style model to synthesize a natural-language answer
7. **Answer & Citations** â€“ final response with stats + where they came from

Example stack (conceptual):

- **Framework:** LangChain / LlamaIndex (not strictly required, but realistic)
- **Embeddings:** OpenAI `text-embedding-3-small`
- **Vector Store:** Chroma
- **LLM:** Hosted GPT-4 / GPT-5.1
- **Orchestration:** Simple Python script or notebook

Detailed YAML and rationale are in the CAPB report.

---

## âš™ï¸ Component Choices & Bakeoff

**Vector DB:**

- **Option A:** FAISS
- **Option B:** Chroma âœ… (selected)

Chroma wins for this project due to:

- Easy Python integration
- Built-in persistence and metadata filters
- Good fit for a small, local academic project

**Reranker:**

- **Option A:** None âœ… (selected)
- **Option B:** Cohere Rerank

For this mini-project, the corpus is small enough that a dedicated reranker is not strictly necessary. The project prioritizes simplicity and time constraints over squeezing out extra retrieval quality.

Other alternatives and tradeoffs are documented in `capb_report.md`.

---

## ğŸ§ª Evaluation Plan

- **Test Set:** ~15 questions spanning:
  - Single-player queries (career, per-season)
  - Team comparisons across seasons
  - â€œLeadersâ€ queries (e.g., highest OPS in a given year)

**Metrics:**

- % Correct Answers
- % Answers with proper citations
- Average latency per query

Example target (baseline):

- 12 / 15 correct
- 12 / 15 with citations
- ~3â€“4 seconds average latency on a local machine

---

## âš ï¸ Risks & Future Work

**Key risks & edge cases:**

- Ambiguous player names (multiple players named â€œSmithâ€)
- Missing / incomplete data (very early seasons, pre-1900)
- LLM hallucinations for stats not in the dataset
- Data drift / season updates if live data is included

**Future improvements:**

- Add a reranker for ambiguous or complex queries
- Integrate live Statcast / advanced metrics more deeply
- Expand to minor leagues or college baseball
- Explore GraphRAG-style reasoning over narratives:
  - Example: â€œHow did roster changes affect team performance year to year?â€

---

## ğŸ“š Files in This Repo

- `README.md` â€“ You are here.
- `reports/capb_report.md` â€“ The completed CAPB mini-project report.
- `reports/rubric.md` â€“ The project rubric for reference and self-check.

This repo is meant as a **documentation + design artifact** for MP#3, and can be used as a starting point for a full implementation later.
